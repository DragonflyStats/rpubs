


## Model Validation}
%http://www.itl.nist.gov/div898/handbook/pmd/##4/pmd44.htm
Model validation is possibly the most important step in the model building sequence. It is also one of the most overlooked. Often the validation of a model seems to consist of nothing more than quoting the R2 statistic from the fit (which measures the fraction of the total variability in the response that is accounted for by the model). Unfortunately, a high R2 value does not guarantee that the model fits the data well. Use of a model that does not fit the data well cannot provide good answers to the underlying engineering or scientific questions under investigation.

### Why Use Residuals?}

If the model fit to the data were correct, the residuals would approximate the random errors that make the relationship between the explanatory variables and the response variable a statistical relationship. Therefore, if the residuals appear to behave randomly, it suggests that the model fits the data well. On the other hand, if non-random structure is evident in the residuals, it is a clear sign that the model fits the data poorly. the section #s listed below detail the types of plots to use to test different aspects of a model and give guidance on the correct interpretations of different results that could be observed for each type of plot.
---------------------------------------------------------------------------------------------------------------------- %
## Introduction to Residuals}

The difference between the observed value of the dependent variable (y) and the predicted value ($\hat{y}$) is called the \textbf{residual} (e). Each data point has one residual.

\[\mbox{Residual} = \mbox{Observed value} - \mbox{Predicted value}\] 
\[e = y - \hat{y}\]

Both the sum and the mean of the residuals are equal to zero. 
%That is, Σ e = 0 and e = 0.

### Residual Plots}
A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate.

Below the table on the left shows inputs and outputs from a simple linear regression analysis, and the chart on the right displays the residual (e) and independent variable (X) as a residual plot.

%x6070808595
%y7065709585
%ŷ65.41171.84978.28881.50787.945
%e4.589-6.849-8.28813.493-2.945
% Image of residual plot
<p>
The residual plot shows a fairly random pattern - the first residual is positive, the next two are negative, the fourth is positive, and the last residual is negative. This random pattern indicates that a linear model provides a decent fit to the data.

Below, the residual plots show three typical patterns. The first plot shows a random pattern, indicating a good fit for a linear model. The other plot patterns are non-random (U-shaped and inverted U), suggesting a better fit for a non-linear model.


%Random patternNon-random: U-shapedNon-random: Inverted U
In the next lesson, we will work on a problem, where the residual plot shows a non-random pattern. And we will show how to "transform" the data to use a linear model with nonlinear data.

------------------------------------------------------------------------------------------
<p>
%http://blog.minitab.com/blog/adventures-in-statistics/why-you-need-to-check-your-residual-plots-for-regression-analysis
In the graph above, you can predict non-zero values for the residuals based on the fitted value. For example, a fitted value of 8 has an expected residual that is negative. Conversely, a fitted value of 5 or 11 has an expected residual that is positive.

The non-random pattern in the residuals indicates that the deterministic portion (predictor variables) of the model is not capturing some explanatory information that is “leaking” into the residuals. The graph could represent several ways in which the model is not explaining all that is possible. 

Possibilities include:


* A missing variable
* A missing higher-order term of a variable in the model to explain the curvature
* A missing interction between terms already in the model



Identifying and fixing the problem so that the predictors now explain the information that they missed before should produce a good-looking set of residuals!

In addition to the above, here are two more specific ways that predictive information can sneak into the residuals:

The residuals should not be correlated with another variable. If you can predict the residuals with another variable, that variable should be included in the model. In Minitab’s regression, you can plot the residuals by other variables to look for this problem.

#### Autocorrelation} 
Adjacent residuals should not be correlated with each other (\textbf{autocorrelation}). If you can use one residual to predict the next residual, there is some predictive information present that is not captured by the predictors. Typically, this situation involves time-ordered observations. For example, if a residual is more likely to be followed by another residual that has the same sign, adjacent residuals are positively correlated. You can include a variable that captures the relevant time-related information, or use a time series analysis. 

In Minitab’s regression, you can perform the \textbf{\textit{Durbin-Watson} }test to test for autocorrelation.
<p>
## Leverage and Influence}
% http://polisci.msu.edu/jacoby/icpsr/regress3/lectures/week3/11.Outliers.pdf

### Summary of Influence Statistics}

* \textbf{Studentized Residuals} – Residuals divided by their estimated standard errors (like t-statistics). Observations with values r than 3 in absolute value are considered outliers.
* \textbf{Leverage Values (Hat Diag)} – Measure of how far an observation is from the others in terms of the levels of the independent variables (not the dependent variable). Observations with values r than $2(k+1)/n$ are considered to be potentially highly influential, where k is the number of predictors and n is the sample size.
* \textbf{DFFITS} – Measure of how much an observation has effected its fitted value from the regression model. Values r than $2\sqrt{(k+1)/n}$ in absolute value are considered highly influential. %Use standardized DFFITS in SPSS.
* \textbf{DFBETAS} – Measure of how much an observation has effected the estimate of a regression coefficient (there is one DFBETA for each regression coefficient, including the intercept). Values r than 2/sqrt(n) in absolute value are considered highly influential.
\\
The measure that measures how much impact each observation has on a particular predictor is DFBETAs The DFBETA for a predictor and for a particular observation is the difference between the regression coefficient calculated for all of the data and the regression coefficient calculated with the observation deleted, scaled by the standard error calculated with the observation deleted. 

* \textbf{Cook’s D} – Measure of aggregate impact of each observation on the group of regression coefficients, as well as the group of fitted values. Values r than 4/n are considered highly influential.



### Influential Observations:
DFBeta and DFBetas}


### Cook's Distance}

------------------------------------------------------------ %
<p>
## Diagnostic Plots for Linear Models with \texttt{R}}
Plot Diagnostics for an \texttt{lm} Object

### Description}

Six plots (selectable by \texttt{which}) are currently available: 
\begin{enumerate}
* a plot of residuals against fitted values, 
* a Scale-Location plot of \textit{sqrt(| residuals |}) against fitted values, 
* a Normal Q-Q plot, 
* a plot of Cook's distances versus row labels, 
* a plot of residuals against leverages, 
* a plot of Cook's distances against leverage/(1-leverage).
\end{enumerate} By default, the first three and 5 are provided.


* 
The \textbf{Scale-Location} plot, also called ‘Spread-Location’ or ‘S-L’ plot, takes the square root of the absolute residuals in order to diminish skewness (sqrt(|E|)) is much less skewed than | E | for Gaussian zero-mean E).

* 
The \textbf{Residual-Leverage} plot shows contours of equal Cook's distance, for values of cook.levels (by default 0.5 and 1) and omits cases with leverage one with a warning. If the leverages are constant (as is typically the case in a balanced aov situation) the plot uses factor level combinations instead of the leverages for the x-axis. (The factor levels are ordered by mean fitted value.)


```{r}
par(mfrow=c(4,1))
plot(fittedmodel)
par(opar)
```

------------------------------------------------------------ %
<p>
## Robust Regression (Optional Section)}

------------------------------------------------------------ %
<p>
## Residual Analysis for GLMs (Optional Section)}

### Pearson and Deviance Residuals} 
% https://v8doc.sas.com/sashtml/insight/chap39/sect55.htm





The \textbf{deviance residual} is the measure of deviance contributed from each observation and is given by
\[r_{Di} = \textrm{sign}( r_{i})
 \sqrt{ d_{i}}\]
where $d_i$ is the individual deviance contribution.
The deviance residuals can be used to check the model fit at each observation for generalized linear models. 

%These residuals are stored in variables named \textit{RD\_yname} for each response variable, where yname is the response variable name. 

The standardized and studentized deviance residuals are
\[
r_{Dsi} = \frac{r_{Di}}{\sqrt{\hat{ \phi} (1- h_{i})} }\]
\[r_{Dti} = \frac{r_{Di}}{\sqrt{ \hat{ \phi}_{(i)}
 (1- h_{i})}}\]
 
 
### Diagnostics for Logistic Regression}

### Diagnostics for Poisson Regression}
------------------------------------------------------------ %
<p>
## Residual Analysis for LME Models}

### Likelihood Distance}



0
